{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f879b044",
   "metadata": {},
   "source": [
    "# CNN 3 Classes\n",
    "ECG Multi-class Classification using CNN\n",
    "Overview\n",
    "Multi-class deep learning model for classifying cardiac conditions from 15-lead ECG signals using ResNet-based 1D CNN architecture.\n",
    "Dataset\n",
    "\n",
    "Source: PTB Diagnostic ECG Database\n",
    "Samples: 15-lead ECG recordings (5000 time points per signal)\n",
    "Classes: 3 cardiac conditions after filtering\n",
    "\n",
    "Healthy Controls\n",
    "Myocardial Infarction\n",
    "Bundle Branch Block\n",
    "\n",
    "\n",
    "Excluded: Myocarditis, Valvular heart disease, Cardiomyopathy/Heart failure, Dysrhythmia\n",
    "Split: Stratified train/validation/test split\n",
    "\n",
    "Key Modifications from Binary Model\n",
    "\n",
    "Multi-class output: Softmax activation instead of sigmoid\n",
    "Loss function: Categorical crossentropy instead of focal loss\n",
    "Label encoding: One-hot encoding for multiple classes\n",
    "SMOTE: Adapted for multi-class imbalance handling\n",
    "Metrics: Multi-class precision, recall, F1-score per class\n",
    "\n",
    "Key Techniques\n",
    "\n",
    "Class Balancing: Multi-class SMOTE oversampling\n",
    "Data Augmentation: Noise injection, amplitude scaling, temporal shifts, mixup\n",
    "Architecture: ResNet with residual blocks (64→128→256→512 filters) + BatchNorm + Dropout\n",
    "Training: Adam optimizer, learning rate scheduling, early stopping (100 epochs max)\n",
    "Regularization: L2 regularization, dropout, batch normalization\n",
    "\n",
    "Model Performance Metrics\n",
    "Accuracy | Precision | Recall | F1-Score | Confusion Matrix (per class)\n",
    "Technologies\n",
    "TensorFlow/Keras | NumPy | Pandas | scikit-learn | imbalanced-learn | wfdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b18318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ECG data...\n",
      "Found 290 patient folders\n",
      "Processed 50/290 patients... Loaded: 162, Skipped: 0\n",
      "Processed 100/290 patients... Loaded: 311, Skipped: 0\n",
      "Processed 150/290 patients... Loaded: 340, Skipped: 27\n",
      "Processed 200/290 patients... Loaded: 382, Skipped: 54\n",
      "Processed 250/290 patients... Loaded: 431, Skipped: 68\n",
      "\n",
      "Total files found: 549\n",
      "Successfully loaded: 465\n",
      "Excluded (Myocarditis & Valvular): 46\n",
      "Skipped (other reasons): 38\n",
      "Success rate: 84.7%\n",
      "\n",
      "X shape: (465, 5000, 15)\n",
      "y shape: (465,)\n",
      "\n",
      "Label distribution:\n",
      "  Bundle branch block: 17 (3.7%)\n",
      "  Healthy controls: 80 (17.2%)\n",
      "  Myocardial infarction: 368 (79.1%)\n",
      "\n",
      "Number of classes: 3\n",
      "Classes: ['Bundle branch block' 'Healthy controls' 'Myocardial infarction']\n",
      "One-hot encoded labels shape: (465, 3)\n",
      "\n",
      "==================================================\n",
      "SPLITTING DATA\n",
      "==================================================\n",
      "Train set: 297 samples\n",
      "  Bundle branch block: 11 (3.7%)\n",
      "  Healthy controls: 51 (17.2%)\n",
      "  Myocardial infarction: 235 (79.1%)\n",
      "\n",
      "Validation set: 75 samples\n",
      "  Bundle branch block: 3 (4.0%)\n",
      "  Healthy controls: 13 (17.3%)\n",
      "  Myocardial infarction: 59 (78.7%)\n",
      "\n",
      "Test set: 93 samples\n",
      "  Bundle branch block: 3 (3.2%)\n",
      "  Healthy controls: 16 (17.2%)\n",
      "  Myocardial infarction: 74 (79.6%)\n",
      "\n",
      "==================================================\n",
      "APPLYING SMOTE FOR CLASS BALANCING\n",
      "==================================================\n",
      "\n",
      "Before SMOTE:\n",
      "  Bundle branch block: 11\n",
      "  Healthy controls: 51\n",
      "  Myocardial infarction: 235\n",
      "\n",
      "After SMOTE:\n",
      "  Bundle branch block: 235\n",
      "  Healthy controls: 235\n",
      "  Myocardial infarction: 235\n",
      "\n",
      "Balanced training set shape: (705, 5000, 15)\n",
      "\n",
      "==================================================\n",
      "BUILDING AND COMPILING MODEL\n",
      "==================================================\n",
      "\n",
      "Class weights:\n",
      "  Bundle branch block: 1.000\n",
      "  Healthy controls: 1.000\n",
      "  Myocardial infarction: 1.000\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 5000, 15)]           0         []                            \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)          (None, 5000, 64)             6784      ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 5000, 64)             256       ['conv1d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_13 (Activation)  (None, 5000, 64)             0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPoolin  (None, 1666, 64)             0         ['activation_13[0][0]']       \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 1666, 64)             0         ['max_pooling1d_4[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)          (None, 1666, 64)             20544     ['dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 1666, 64)             256       ['conv1d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_14 (Activation)  (None, 1666, 64)             0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 1666, 64)             0         ['activation_14[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)          (None, 1666, 64)             20544     ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 1666, 64)             256       ['conv1d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 1666, 64)             0         ['batch_normalization_15[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      " activation_15 (Activation)  (None, 1666, 64)             0         ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPoolin  (None, 555, 64)              0         ['activation_15[0][0]']       \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)          (None, 555, 128)             41088     ['max_pooling1d_5[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 555, 128)             512       ['conv1d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_16 (Activation)  (None, 555, 128)             0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 555, 128)             0         ['activation_16[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)          (None, 555, 128)             82048     ['dropout_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 555, 128)             512       ['conv1d_20[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)          (None, 555, 128)             8320      ['max_pooling1d_5[0][0]']     \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 555, 128)             0         ['batch_normalization_17[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv1d_21[0][0]']           \n",
      "                                                                                                  \n",
      " activation_17 (Activation)  (None, 555, 128)             0         ['add_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)          (None, 555, 128)             82048     ['activation_17[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 555, 128)             512       ['conv1d_22[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_18 (Activation)  (None, 555, 128)             0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 555, 128)             0         ['activation_18[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)          (None, 555, 128)             82048     ['dropout_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 555, 128)             512       ['conv1d_23[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 555, 128)             0         ['batch_normalization_19[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'activation_17[0][0]']       \n",
      "                                                                                                  \n",
      " activation_19 (Activation)  (None, 555, 128)             0         ['add_8[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPoolin  (None, 185, 128)             0         ['activation_19[0][0]']       \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)          (None, 185, 256)             98560     ['max_pooling1d_6[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 185, 256)             1024      ['conv1d_24[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_20 (Activation)  (None, 185, 256)             0         ['batch_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 185, 256)             0         ['activation_20[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)          (None, 185, 256)             196864    ['dropout_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 185, 256)             1024      ['conv1d_25[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)          (None, 185, 256)             33024     ['max_pooling1d_6[0][0]']     \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 185, 256)             0         ['batch_normalization_21[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv1d_26[0][0]']           \n",
      "                                                                                                  \n",
      " activation_21 (Activation)  (None, 185, 256)             0         ['add_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)          (None, 185, 256)             196864    ['activation_21[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 185, 256)             1024      ['conv1d_27[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_22 (Activation)  (None, 185, 256)             0         ['batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 185, 256)             0         ['activation_22[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)          (None, 185, 256)             196864    ['dropout_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, 185, 256)             1024      ['conv1d_28[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 185, 256)             0         ['batch_normalization_23[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'activation_21[0][0]']       \n",
      "                                                                                                  \n",
      " activation_23 (Activation)  (None, 185, 256)             0         ['add_10[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPoolin  (None, 61, 256)              0         ['activation_23[0][0]']       \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)          (None, 61, 512)              393728    ['max_pooling1d_7[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_24 (Ba  (None, 61, 512)              2048      ['conv1d_29[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_24 (Activation)  (None, 61, 512)              0         ['batch_normalization_24[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 61, 512)              0         ['activation_24[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)          (None, 61, 512)              786944    ['dropout_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (Ba  (None, 61, 512)              2048      ['conv1d_30[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)          (None, 61, 512)              131584    ['max_pooling1d_7[0][0]']     \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 61, 512)              0         ['batch_normalization_25[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv1d_31[0][0]']           \n",
      "                                                                                                  \n",
      " activation_25 (Activation)  (None, 61, 512)              0         ['add_11[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 512)                  0         ['activation_25[0][0]']       \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 256)                  131328    ['global_average_pooling1d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)        (None, 256)                  0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 128)                  32896     ['dropout_16[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)        (None, 128)                  0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 3)                    387       ['dropout_17[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2553475 (9.74 MB)\n",
      "Trainable params: 2547971 (9.72 MB)\n",
      "Non-trainable params: 5504 (21.50 KB)\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "==================================================\n",
      "TRAINING MODEL\n",
      "==================================================\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 4.0508 - accuracy: 0.4752 - precision_1: 0.5691 - recall_1: 0.3562\n",
      "Epoch 1: val_accuracy improved from -inf to 0.78667, saving model to models\\best_model_multiclass.keras\n",
      "23/23 [==============================] - 27s 754ms/step - loss: 4.0508 - accuracy: 0.4752 - precision_1: 0.5691 - recall_1: 0.3562 - val_loss: 3.5076 - val_accuracy: 0.7867 - val_precision_1: 0.7867 - val_recall_1: 0.7867 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 3.2738 - accuracy: 0.7177 - precision_1: 0.7824 - recall_1: 0.5502\n",
      "Epoch 2: val_accuracy did not improve from 0.78667\n",
      "23/23 [==============================] - 16s 682ms/step - loss: 3.2738 - accuracy: 0.7177 - precision_1: 0.7824 - recall_1: 0.5502 - val_loss: 3.3637 - val_accuracy: 0.7867 - val_precision_1: 0.7867 - val_recall_1: 0.7867 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.9529 - accuracy: 0.7461 - precision_1: 0.8170 - recall_1: 0.6302\n",
      "Epoch 3: val_accuracy improved from 0.78667 to 0.80000, saving model to models\\best_model_multiclass.keras\n",
      "23/23 [==============================] - 30s 1s/step - loss: 2.9529 - accuracy: 0.7461 - precision_1: 0.8170 - recall_1: 0.6302 - val_loss: 2.7456 - val_accuracy: 0.8000 - val_precision_1: 0.8082 - val_recall_1: 0.7867 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.5370 - accuracy: 0.8340 - precision_1: 0.8866 - recall_1: 0.6773\n",
      "Epoch 4: val_accuracy did not improve from 0.80000\n",
      "23/23 [==============================] - 28s 1s/step - loss: 2.5370 - accuracy: 0.8340 - precision_1: 0.8866 - recall_1: 0.6773 - val_loss: 2.5512 - val_accuracy: 0.7067 - val_precision_1: 0.7500 - val_recall_1: 0.6400 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.3100 - accuracy: 0.8355 - precision_1: 0.8733 - recall_1: 0.7019\n",
      "Epoch 5: val_accuracy did not improve from 0.80000\n",
      "23/23 [==============================] - 28s 1s/step - loss: 2.3100 - accuracy: 0.8355 - precision_1: 0.8733 - recall_1: 0.7019 - val_loss: 2.5302 - val_accuracy: 0.7733 - val_precision_1: 0.7808 - val_recall_1: 0.7600 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.0908 - accuracy: 0.8340 - precision_1: 0.8731 - recall_1: 0.7162\n",
      "Epoch 6: val_accuracy improved from 0.80000 to 0.85333, saving model to models\\best_model_multiclass.keras\n",
      "23/23 [==============================] - 29s 1s/step - loss: 2.0908 - accuracy: 0.8340 - precision_1: 0.8731 - recall_1: 0.7162 - val_loss: 2.0304 - val_accuracy: 0.8533 - val_precision_1: 0.8514 - val_recall_1: 0.8400 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.9879 - accuracy: 0.8326 - precision_1: 0.8823 - recall_1: 0.6902\n",
      "Epoch 7: val_accuracy did not improve from 0.85333\n",
      "23/23 [==============================] - 25s 1s/step - loss: 1.9879 - accuracy: 0.8326 - precision_1: 0.8823 - recall_1: 0.6902 - val_loss: 2.4140 - val_accuracy: 0.6133 - val_precision_1: 0.6618 - val_recall_1: 0.6000 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7796 - accuracy: 0.8695 - precision_1: 0.8988 - recall_1: 0.7143\n",
      "Epoch 8: val_accuracy did not improve from 0.85333\n",
      "23/23 [==============================] - 23s 983ms/step - loss: 1.7796 - accuracy: 0.8695 - precision_1: 0.8988 - recall_1: 0.7143 - val_loss: 2.2068 - val_accuracy: 0.6533 - val_precision_1: 0.6571 - val_recall_1: 0.6133 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7039 - accuracy: 0.8326 - precision_1: 0.8628 - recall_1: 0.6491\n",
      "Epoch 9: val_accuracy did not improve from 0.85333\n",
      "23/23 [==============================] - 23s 998ms/step - loss: 1.7039 - accuracy: 0.8326 - precision_1: 0.8628 - recall_1: 0.6491 - val_loss: 3.0780 - val_accuracy: 0.4133 - val_precision_1: 0.3971 - val_recall_1: 0.3600 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5027 - accuracy: 0.8780 - precision_1: 0.9106 - recall_1: 0.7538\n",
      "Epoch 10: val_accuracy did not improve from 0.85333\n",
      "23/23 [==============================] - 24s 1s/step - loss: 1.5027 - accuracy: 0.8780 - precision_1: 0.9106 - recall_1: 0.7538 - val_loss: 1.9872 - val_accuracy: 0.6533 - val_precision_1: 0.7015 - val_recall_1: 0.6267 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4347 - accuracy: 0.8766 - precision_1: 0.9098 - recall_1: 0.7432\n",
      "Epoch 11: val_accuracy did not improve from 0.85333\n",
      "23/23 [==============================] - 24s 1s/step - loss: 1.4347 - accuracy: 0.8766 - precision_1: 0.9098 - recall_1: 0.7432 - val_loss: 7.9332 - val_accuracy: 0.1733 - val_precision_1: 0.1757 - val_recall_1: 0.1733 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3100 - accuracy: 0.8794 - precision_1: 0.9112 - recall_1: 0.7476\n",
      "Epoch 12: val_accuracy did not improve from 0.85333\n",
      "23/23 [==============================] - 24s 1s/step - loss: 1.3100 - accuracy: 0.8794 - precision_1: 0.9112 - recall_1: 0.7476 - val_loss: 3.2568 - val_accuracy: 0.4667 - val_precision_1: 0.4493 - val_recall_1: 0.4133 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1954 - accuracy: 0.9078 - precision_1: 0.9262 - recall_1: 0.8071\n",
      "Epoch 13: val_accuracy did not improve from 0.85333\n",
      "23/23 [==============================] - 17s 700ms/step - loss: 1.1954 - accuracy: 0.9078 - precision_1: 0.9262 - recall_1: 0.8071 - val_loss: 2.2675 - val_accuracy: 0.5467 - val_precision_1: 0.5522 - val_recall_1: 0.4933 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.2266 - accuracy: 0.8908 - precision_1: 0.9383 - recall_1: 0.7222\n",
      "Epoch 14: val_accuracy improved from 0.85333 to 0.89333, saving model to models\\best_model_multiclass.keras\n",
      "23/23 [==============================] - 15s 667ms/step - loss: 1.2266 - accuracy: 0.8908 - precision_1: 0.9383 - recall_1: 0.7222 - val_loss: 1.1334 - val_accuracy: 0.8933 - val_precision_1: 0.8904 - val_recall_1: 0.8667 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1548 - accuracy: 0.8908 - precision_1: 0.9188 - recall_1: 0.7378\n",
      "Epoch 15: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 16s 692ms/step - loss: 1.1548 - accuracy: 0.8908 - precision_1: 0.9188 - recall_1: 0.7378 - val_loss: 2.3677 - val_accuracy: 0.4400 - val_precision_1: 0.4242 - val_recall_1: 0.3733 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0043 - accuracy: 0.9149 - precision_1: 0.9333 - recall_1: 0.8353\n",
      "Epoch 16: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 20s 859ms/step - loss: 1.0043 - accuracy: 0.9149 - precision_1: 0.9333 - recall_1: 0.8353 - val_loss: 2.4767 - val_accuracy: 0.4000 - val_precision_1: 0.4143 - val_recall_1: 0.3867 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0823 - accuracy: 0.8709 - precision_1: 0.8969 - recall_1: 0.7435\n",
      "Epoch 17: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 20s 885ms/step - loss: 1.0823 - accuracy: 0.8709 - precision_1: 0.8969 - recall_1: 0.7435 - val_loss: 1.8698 - val_accuracy: 0.6133 - val_precision_1: 0.6111 - val_recall_1: 0.5867 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0396 - accuracy: 0.9106 - precision_1: 0.9283 - recall_1: 0.7521\n",
      "Epoch 18: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 19s 828ms/step - loss: 1.0396 - accuracy: 0.9106 - precision_1: 0.9283 - recall_1: 0.7521 - val_loss: 1.2228 - val_accuracy: 0.8000 - val_precision_1: 0.8286 - val_recall_1: 0.7733 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9716 - accuracy: 0.9248 - precision_1: 0.9556 - recall_1: 0.7026\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 20s 884ms/step - loss: 0.9716 - accuracy: 0.9248 - precision_1: 0.9556 - recall_1: 0.7026 - val_loss: 1.5288 - val_accuracy: 0.5867 - val_precision_1: 0.6212 - val_recall_1: 0.5467 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8967 - accuracy: 0.9404 - precision_1: 0.9520 - recall_1: 0.8342\n",
      "Epoch 20: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 19s 831ms/step - loss: 0.8967 - accuracy: 0.9404 - precision_1: 0.9520 - recall_1: 0.8342 - val_loss: 2.0232 - val_accuracy: 0.5067 - val_precision_1: 0.5294 - val_recall_1: 0.4800 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9549 - accuracy: 0.8950 - precision_1: 0.9453 - recall_1: 0.7485\n",
      "Epoch 21: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 21s 883ms/step - loss: 0.9549 - accuracy: 0.8950 - precision_1: 0.9453 - recall_1: 0.7485 - val_loss: 1.3293 - val_accuracy: 0.6667 - val_precision_1: 0.7031 - val_recall_1: 0.6000 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9162 - accuracy: 0.9362 - precision_1: 0.9644 - recall_1: 0.7143\n",
      "Epoch 22: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 23s 996ms/step - loss: 0.9162 - accuracy: 0.9362 - precision_1: 0.9644 - recall_1: 0.7143 - val_loss: 2.0089 - val_accuracy: 0.3867 - val_precision_1: 0.3607 - val_recall_1: 0.2933 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7947 - accuracy: 0.9546 - precision_1: 0.9718 - recall_1: 0.7733\n",
      "Epoch 23: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 21s 888ms/step - loss: 0.7947 - accuracy: 0.9546 - precision_1: 0.9718 - recall_1: 0.7733 - val_loss: 1.2696 - val_accuracy: 0.7067 - val_precision_1: 0.7869 - val_recall_1: 0.6400 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8325 - accuracy: 0.9433 - precision_1: 0.9537 - recall_1: 0.7330\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 20s 847ms/step - loss: 0.8325 - accuracy: 0.9433 - precision_1: 0.9537 - recall_1: 0.7330 - val_loss: 1.2238 - val_accuracy: 0.6800 - val_precision_1: 0.7000 - val_recall_1: 0.6533 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7569 - accuracy: 0.9447 - precision_1: 0.9668 - recall_1: 0.8394\n",
      "Epoch 25: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 20s 842ms/step - loss: 0.7569 - accuracy: 0.9447 - precision_1: 0.9668 - recall_1: 0.8394 - val_loss: 1.2955 - val_accuracy: 0.6933 - val_precision_1: 0.6970 - val_recall_1: 0.6133 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8448 - accuracy: 0.9560 - precision_1: 0.9753 - recall_1: 0.7671\n",
      "Epoch 26: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 19s 830ms/step - loss: 0.8448 - accuracy: 0.9560 - precision_1: 0.9753 - recall_1: 0.7671 - val_loss: 1.2365 - val_accuracy: 0.6933 - val_precision_1: 0.7143 - val_recall_1: 0.6000 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8127 - accuracy: 0.9475 - precision_1: 0.9737 - recall_1: 0.7605\n",
      "Epoch 27: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 19s 808ms/step - loss: 0.8127 - accuracy: 0.9475 - precision_1: 0.9737 - recall_1: 0.7605 - val_loss: 1.1951 - val_accuracy: 0.7200 - val_precision_1: 0.7619 - val_recall_1: 0.6400 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6786 - accuracy: 0.9645 - precision_1: 0.9739 - recall_1: 0.8692\n",
      "Epoch 28: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 19s 819ms/step - loss: 0.6786 - accuracy: 0.9645 - precision_1: 0.9739 - recall_1: 0.8692 - val_loss: 1.4894 - val_accuracy: 0.5467 - val_precision_1: 0.5735 - val_recall_1: 0.5200 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7258 - accuracy: 0.9461 - precision_1: 0.9651 - recall_1: 0.7427\n",
      "Epoch 29: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 19s 818ms/step - loss: 0.7258 - accuracy: 0.9461 - precision_1: 0.9651 - recall_1: 0.7427 - val_loss: 0.9214 - val_accuracy: 0.8800 - val_precision_1: 0.9130 - val_recall_1: 0.8400 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7009 - accuracy: 0.9418 - precision_1: 0.9540 - recall_1: 0.8177\n",
      "Epoch 30: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 18s 795ms/step - loss: 0.7019 - accuracy: 0.9404 - precision_1: 0.9540 - recall_1: 0.8167 - val_loss: 0.9137 - val_accuracy: 0.8667 - val_precision_1: 0.8732 - val_recall_1: 0.8267 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7419 - accuracy: 0.9433 - precision_1: 0.9713 - recall_1: 0.7609\n",
      "Epoch 31: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 16s 693ms/step - loss: 0.7419 - accuracy: 0.9433 - precision_1: 0.9713 - recall_1: 0.7609 - val_loss: 1.2777 - val_accuracy: 0.6933 - val_precision_1: 0.7302 - val_recall_1: 0.6133 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7498 - accuracy: 0.9518 - precision_1: 0.9711 - recall_1: 0.7839\n",
      "Epoch 32: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 19s 801ms/step - loss: 0.7498 - accuracy: 0.9518 - precision_1: 0.9711 - recall_1: 0.7839 - val_loss: 1.0172 - val_accuracy: 0.8133 - val_precision_1: 0.8194 - val_recall_1: 0.7867 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7203 - accuracy: 0.9518 - precision_1: 0.9755 - recall_1: 0.8050\n",
      "Epoch 33: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 19s 798ms/step - loss: 0.7203 - accuracy: 0.9518 - precision_1: 0.9755 - recall_1: 0.8050 - val_loss: 1.3356 - val_accuracy: 0.6267 - val_precision_1: 0.6567 - val_recall_1: 0.5867 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8071 - accuracy: 0.9504 - precision_1: 0.9666 - recall_1: 0.7040\n",
      "Epoch 34: val_accuracy did not improve from 0.89333\n",
      "23/23 [==============================] - 18s 796ms/step - loss: 0.8071 - accuracy: 0.9504 - precision_1: 0.9666 - recall_1: 0.7040 - val_loss: 0.8577 - val_accuracy: 0.8800 - val_precision_1: 0.8939 - val_recall_1: 0.7867 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6628 - accuracy: 0.9631 - precision_1: 0.9752 - recall_1: 0.7433\n",
      "Epoch 35: val_accuracy improved from 0.89333 to 0.90667, saving model to models\\best_model_multiclass.keras\n",
      "23/23 [==============================] - 19s 811ms/step - loss: 0.6628 - accuracy: 0.9631 - precision_1: 0.9752 - recall_1: 0.7433 - val_loss: 0.7728 - val_accuracy: 0.9067 - val_precision_1: 0.9178 - val_recall_1: 0.8933 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.9674 - precision_1: 0.9856 - recall_1: 0.8054\n",
      "Epoch 36: val_accuracy did not improve from 0.90667\n",
      "23/23 [==============================] - 19s 833ms/step - loss: 0.6932 - accuracy: 0.9674 - precision_1: 0.9856 - recall_1: 0.8054 - val_loss: 0.8355 - val_accuracy: 0.8933 - val_precision_1: 0.9041 - val_recall_1: 0.8800 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7673 - accuracy: 0.9461 - precision_1: 0.9767 - recall_1: 0.7304\n",
      "Epoch 37: val_accuracy did not improve from 0.90667\n",
      "23/23 [==============================] - 19s 799ms/step - loss: 0.7673 - accuracy: 0.9461 - precision_1: 0.9767 - recall_1: 0.7304 - val_loss: 0.8453 - val_accuracy: 0.8800 - val_precision_1: 0.9143 - val_recall_1: 0.8533 - lr: 2.5000e-04\n",
      "Epoch 38/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6457 - accuracy: 0.9674 - precision_1: 0.9856 - recall_1: 0.7799\n",
      "Epoch 38: val_accuracy did not improve from 0.90667\n",
      "23/23 [==============================] - 19s 813ms/step - loss: 0.6457 - accuracy: 0.9674 - precision_1: 0.9856 - recall_1: 0.7799 - val_loss: 1.0953 - val_accuracy: 0.7600 - val_precision_1: 0.7969 - val_recall_1: 0.6800 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7230 - accuracy: 0.9574 - precision_1: 0.9754 - recall_1: 0.7401\n",
      "Epoch 39: val_accuracy improved from 0.90667 to 0.93333, saving model to models\\best_model_multiclass.keras\n",
      "23/23 [==============================] - 19s 830ms/step - loss: 0.7230 - accuracy: 0.9574 - precision_1: 0.9754 - recall_1: 0.7401 - val_loss: 0.8113 - val_accuracy: 0.9333 - val_precision_1: 0.9403 - val_recall_1: 0.8400 - lr: 2.5000e-04\n",
      "Epoch 40/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6263 - accuracy: 0.9730 - precision_1: 0.9784 - recall_1: 0.8230\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 21s 902ms/step - loss: 0.6263 - accuracy: 0.9730 - precision_1: 0.9784 - recall_1: 0.8230 - val_loss: 0.9426 - val_accuracy: 0.8533 - val_precision_1: 0.8529 - val_recall_1: 0.7733 - lr: 2.5000e-04\n",
      "Epoch 41/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6253 - accuracy: 0.9532 - precision_1: 0.9712 - recall_1: 0.7475\n",
      "Epoch 41: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 860ms/step - loss: 0.6253 - accuracy: 0.9532 - precision_1: 0.9712 - recall_1: 0.7475 - val_loss: 0.9374 - val_accuracy: 0.8267 - val_precision_1: 0.8333 - val_recall_1: 0.8000 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6258 - accuracy: 0.9702 - precision_1: 0.9783 - recall_1: 0.7256\n",
      "Epoch 42: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 809ms/step - loss: 0.6258 - accuracy: 0.9702 - precision_1: 0.9783 - recall_1: 0.7256 - val_loss: 0.8318 - val_accuracy: 0.8667 - val_precision_1: 0.8889 - val_recall_1: 0.8533 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5286 - accuracy: 0.9773 - precision_1: 0.9842 - recall_1: 0.8545\n",
      "Epoch 43: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 826ms/step - loss: 0.5286 - accuracy: 0.9773 - precision_1: 0.9842 - recall_1: 0.8545 - val_loss: 1.1676 - val_accuracy: 0.7733 - val_precision_1: 0.7945 - val_recall_1: 0.7733 - lr: 1.2500e-04\n",
      "Epoch 44/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6206 - accuracy: 0.9773 - precision_1: 0.9856 - recall_1: 0.7761\n",
      "Epoch 44: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 858ms/step - loss: 0.6209 - accuracy: 0.9773 - precision_1: 0.9856 - recall_1: 0.7764 - val_loss: 0.9635 - val_accuracy: 0.8133 - val_precision_1: 0.8310 - val_recall_1: 0.7867 - lr: 1.2500e-04\n",
      "Epoch 45/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5898 - accuracy: 0.9730 - precision_1: 0.9772 - recall_1: 0.8415\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 857ms/step - loss: 0.5898 - accuracy: 0.9730 - precision_1: 0.9772 - recall_1: 0.8415 - val_loss: 0.9059 - val_accuracy: 0.8267 - val_precision_1: 0.8243 - val_recall_1: 0.8133 - lr: 1.2500e-04\n",
      "Epoch 46/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6469 - accuracy: 0.9773 - precision_1: 0.9900 - recall_1: 0.8363\n",
      "Epoch 46: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 878ms/step - loss: 0.6469 - accuracy: 0.9773 - precision_1: 0.9900 - recall_1: 0.8363 - val_loss: 0.7688 - val_accuracy: 0.8933 - val_precision_1: 0.8919 - val_recall_1: 0.8800 - lr: 6.2500e-05\n",
      "Epoch 47/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4997 - accuracy: 0.9844 - precision_1: 0.9844 - recall_1: 0.8715\n",
      "Epoch 47: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 811ms/step - loss: 0.4997 - accuracy: 0.9844 - precision_1: 0.9844 - recall_1: 0.8715 - val_loss: 0.7697 - val_accuracy: 0.8800 - val_precision_1: 0.9041 - val_recall_1: 0.8800 - lr: 6.2500e-05\n",
      "Epoch 48/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6329 - accuracy: 0.9617 - precision_1: 0.9772 - recall_1: 0.7949\n",
      "Epoch 48: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 855ms/step - loss: 0.6329 - accuracy: 0.9617 - precision_1: 0.9772 - recall_1: 0.7949 - val_loss: 0.9163 - val_accuracy: 0.8400 - val_precision_1: 0.8451 - val_recall_1: 0.8000 - lr: 6.2500e-05\n",
      "Epoch 49/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6171 - accuracy: 0.9532 - precision_1: 0.9700 - recall_1: 0.8120\n",
      "Epoch 49: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 815ms/step - loss: 0.6171 - accuracy: 0.9532 - precision_1: 0.9700 - recall_1: 0.8120 - val_loss: 0.7945 - val_accuracy: 0.8933 - val_precision_1: 0.9155 - val_recall_1: 0.8667 - lr: 6.2500e-05\n",
      "Epoch 50/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6091 - accuracy: 0.9716 - precision_1: 0.9871 - recall_1: 0.7704\n",
      "Epoch 50: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 880ms/step - loss: 0.6091 - accuracy: 0.9716 - precision_1: 0.9871 - recall_1: 0.7704 - val_loss: 0.8054 - val_accuracy: 0.8667 - val_precision_1: 0.8873 - val_recall_1: 0.8400 - lr: 6.2500e-05\n",
      "Epoch 51/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6155 - accuracy: 0.9716 - precision_1: 0.9857 - recall_1: 0.8319\n",
      "Epoch 51: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 851ms/step - loss: 0.6155 - accuracy: 0.9716 - precision_1: 0.9857 - recall_1: 0.8319 - val_loss: 0.7569 - val_accuracy: 0.9067 - val_precision_1: 0.9054 - val_recall_1: 0.8933 - lr: 6.2500e-05\n",
      "Epoch 52/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5527 - accuracy: 0.9872 - precision_1: 0.9914 - recall_1: 0.8849\n",
      "Epoch 52: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 815ms/step - loss: 0.5527 - accuracy: 0.9872 - precision_1: 0.9914 - recall_1: 0.8849 - val_loss: 0.7672 - val_accuracy: 0.8933 - val_precision_1: 0.9178 - val_recall_1: 0.8933 - lr: 6.2500e-05\n",
      "Epoch 53/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5603 - accuracy: 0.9759 - precision_1: 0.9900 - recall_1: 0.8771\n",
      "Epoch 53: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 822ms/step - loss: 0.5603 - accuracy: 0.9759 - precision_1: 0.9900 - recall_1: 0.8771 - val_loss: 0.7490 - val_accuracy: 0.9067 - val_precision_1: 0.9178 - val_recall_1: 0.8933 - lr: 6.2500e-05\n",
      "Epoch 54/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5480 - accuracy: 0.9816 - precision_1: 0.9829 - recall_1: 0.8627\n",
      "Epoch 54: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 832ms/step - loss: 0.5480 - accuracy: 0.9816 - precision_1: 0.9829 - recall_1: 0.8627 - val_loss: 0.7195 - val_accuracy: 0.9200 - val_precision_1: 0.9189 - val_recall_1: 0.9067 - lr: 6.2500e-05\n",
      "Epoch 55/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5441 - accuracy: 0.9716 - precision_1: 0.9772 - recall_1: 0.8426\n",
      "Epoch 55: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 816ms/step - loss: 0.5441 - accuracy: 0.9716 - precision_1: 0.9772 - recall_1: 0.8426 - val_loss: 0.7509 - val_accuracy: 0.8933 - val_precision_1: 0.9167 - val_recall_1: 0.8800 - lr: 6.2500e-05\n",
      "Epoch 56/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.9872 - precision_1: 0.9900 - recall_1: 0.8292\n",
      "Epoch 56: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 843ms/step - loss: 0.5127 - accuracy: 0.9872 - precision_1: 0.9900 - recall_1: 0.8292 - val_loss: 0.7535 - val_accuracy: 0.8933 - val_precision_1: 0.9167 - val_recall_1: 0.8800 - lr: 6.2500e-05\n",
      "Epoch 57/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5901 - accuracy: 0.9801 - precision_1: 0.9857 - recall_1: 0.7943\n",
      "Epoch 57: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 851ms/step - loss: 0.5901 - accuracy: 0.9801 - precision_1: 0.9857 - recall_1: 0.7943 - val_loss: 0.7823 - val_accuracy: 0.8800 - val_precision_1: 0.8919 - val_recall_1: 0.8800 - lr: 6.2500e-05\n",
      "Epoch 58/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.9887 - precision_1: 0.9901 - recall_1: 0.9052\n",
      "Epoch 58: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 17s 715ms/step - loss: 0.4752 - accuracy: 0.9887 - precision_1: 0.9901 - recall_1: 0.9052 - val_loss: 0.7543 - val_accuracy: 0.8933 - val_precision_1: 0.9041 - val_recall_1: 0.8800 - lr: 6.2500e-05\n",
      "Epoch 59/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6152 - accuracy: 0.9631 - precision_1: 0.9842 - recall_1: 0.8481\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 851ms/step - loss: 0.6152 - accuracy: 0.9631 - precision_1: 0.9842 - recall_1: 0.8481 - val_loss: 0.8241 - val_accuracy: 0.8667 - val_precision_1: 0.9130 - val_recall_1: 0.8400 - lr: 6.2500e-05\n",
      "Epoch 60/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5187 - accuracy: 0.9787 - precision_1: 0.9871 - recall_1: 0.8479\n",
      "Epoch 60: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 865ms/step - loss: 0.5187 - accuracy: 0.9787 - precision_1: 0.9871 - recall_1: 0.8479 - val_loss: 0.8249 - val_accuracy: 0.8800 - val_precision_1: 0.9028 - val_recall_1: 0.8667 - lr: 3.1250e-05\n",
      "Epoch 61/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5468 - accuracy: 0.9887 - precision_1: 0.9929 - recall_1: 0.7961\n",
      "Epoch 61: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 824ms/step - loss: 0.5468 - accuracy: 0.9887 - precision_1: 0.9929 - recall_1: 0.7961 - val_loss: 0.7710 - val_accuracy: 0.8933 - val_precision_1: 0.9178 - val_recall_1: 0.8933 - lr: 3.1250e-05\n",
      "Epoch 62/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6041 - accuracy: 0.9759 - precision_1: 0.9872 - recall_1: 0.7492\n",
      "Epoch 62: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 813ms/step - loss: 0.6041 - accuracy: 0.9759 - precision_1: 0.9872 - recall_1: 0.7492 - val_loss: 0.8058 - val_accuracy: 0.8933 - val_precision_1: 0.8933 - val_recall_1: 0.8933 - lr: 3.1250e-05\n",
      "Epoch 63/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5944 - accuracy: 0.9872 - precision_1: 0.9915 - recall_1: 0.8093\n",
      "Epoch 63: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 822ms/step - loss: 0.5944 - accuracy: 0.9872 - precision_1: 0.9915 - recall_1: 0.8093 - val_loss: 0.8062 - val_accuracy: 0.8933 - val_precision_1: 0.9028 - val_recall_1: 0.8667 - lr: 3.1250e-05\n",
      "Epoch 64/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5929 - accuracy: 0.9830 - precision_1: 0.9886 - recall_1: 0.7697\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 806ms/step - loss: 0.5929 - accuracy: 0.9830 - precision_1: 0.9886 - recall_1: 0.7697 - val_loss: 0.7520 - val_accuracy: 0.9067 - val_precision_1: 0.9167 - val_recall_1: 0.8800 - lr: 3.1250e-05\n",
      "Epoch 65/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.9830 - precision_1: 0.9872 - recall_1: 0.8462\n",
      "Epoch 65: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 828ms/step - loss: 0.5123 - accuracy: 0.9830 - precision_1: 0.9872 - recall_1: 0.8462 - val_loss: 0.7391 - val_accuracy: 0.9200 - val_precision_1: 0.9178 - val_recall_1: 0.8933 - lr: 1.5625e-05\n",
      "Epoch 66/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5440 - accuracy: 0.9887 - precision_1: 0.9943 - recall_1: 0.7841\n",
      "Epoch 66: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 847ms/step - loss: 0.5440 - accuracy: 0.9887 - precision_1: 0.9943 - recall_1: 0.7841 - val_loss: 0.7244 - val_accuracy: 0.9200 - val_precision_1: 0.9315 - val_recall_1: 0.9067 - lr: 1.5625e-05\n",
      "Epoch 67/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5156 - accuracy: 0.9773 - precision_1: 0.9872 - recall_1: 0.7886\n",
      "Epoch 67: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 816ms/step - loss: 0.5156 - accuracy: 0.9773 - precision_1: 0.9872 - recall_1: 0.7886 - val_loss: 0.7155 - val_accuracy: 0.9200 - val_precision_1: 0.9315 - val_recall_1: 0.9067 - lr: 1.5625e-05\n",
      "Epoch 68/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5934 - accuracy: 0.9844 - precision_1: 0.9900 - recall_1: 0.7726\n",
      "Epoch 68: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 799ms/step - loss: 0.5934 - accuracy: 0.9844 - precision_1: 0.9900 - recall_1: 0.7726 - val_loss: 0.7061 - val_accuracy: 0.9333 - val_precision_1: 0.9324 - val_recall_1: 0.9200 - lr: 1.5625e-05\n",
      "Epoch 69/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5325 - accuracy: 0.9589 - precision_1: 0.9886 - recall_1: 0.8573\n",
      "Epoch 69: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 822ms/step - loss: 0.5325 - accuracy: 0.9589 - precision_1: 0.9886 - recall_1: 0.8573 - val_loss: 0.7142 - val_accuracy: 0.9200 - val_precision_1: 0.9324 - val_recall_1: 0.9200 - lr: 1.5625e-05\n",
      "Epoch 70/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6604 - accuracy: 0.9603 - precision_1: 0.9856 - recall_1: 0.7935\n",
      "Epoch 70: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 814ms/step - loss: 0.6604 - accuracy: 0.9603 - precision_1: 0.9856 - recall_1: 0.7935 - val_loss: 0.7266 - val_accuracy: 0.9200 - val_precision_1: 0.9306 - val_recall_1: 0.8933 - lr: 1.5625e-05\n",
      "Epoch 71/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7242 - accuracy: 0.9688 - precision_1: 0.9885 - recall_1: 0.7667\n",
      "Epoch 71: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 837ms/step - loss: 0.7242 - accuracy: 0.9688 - precision_1: 0.9885 - recall_1: 0.7667 - val_loss: 0.6989 - val_accuracy: 0.9333 - val_precision_1: 0.9324 - val_recall_1: 0.9200 - lr: 1.5625e-05\n",
      "Epoch 72/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5038 - accuracy: 0.9887 - precision_1: 0.9900 - recall_1: 0.8074\n",
      "Epoch 72: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 825ms/step - loss: 0.5038 - accuracy: 0.9887 - precision_1: 0.9900 - recall_1: 0.8074 - val_loss: 0.7101 - val_accuracy: 0.9200 - val_precision_1: 0.9324 - val_recall_1: 0.9200 - lr: 1.5625e-05\n",
      "Epoch 73/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.9929 - precision_1: 0.9943 - recall_1: 0.9319\n",
      "Epoch 73: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 17s 715ms/step - loss: 0.4621 - accuracy: 0.9929 - precision_1: 0.9943 - recall_1: 0.9319 - val_loss: 0.7229 - val_accuracy: 0.9200 - val_precision_1: 0.9315 - val_recall_1: 0.9067 - lr: 1.5625e-05\n",
      "Epoch 74/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5718 - accuracy: 0.9759 - precision_1: 0.9957 - recall_1: 0.8107\n",
      "Epoch 74: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 804ms/step - loss: 0.5718 - accuracy: 0.9759 - precision_1: 0.9957 - recall_1: 0.8107 - val_loss: 0.7218 - val_accuracy: 0.9200 - val_precision_1: 0.9306 - val_recall_1: 0.8933 - lr: 1.5625e-05\n",
      "Epoch 75/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5106 - accuracy: 0.9816 - precision_1: 0.9858 - recall_1: 0.7694\n",
      "Epoch 75: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 799ms/step - loss: 0.5106 - accuracy: 0.9816 - precision_1: 0.9858 - recall_1: 0.7694 - val_loss: 0.7432 - val_accuracy: 0.9067 - val_precision_1: 0.9054 - val_recall_1: 0.8933 - lr: 1.5625e-05\n",
      "Epoch 76/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4935 - accuracy: 0.9844 - precision_1: 0.9901 - recall_1: 0.8058\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 76: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 807ms/step - loss: 0.4935 - accuracy: 0.9844 - precision_1: 0.9901 - recall_1: 0.8058 - val_loss: 0.7253 - val_accuracy: 0.9067 - val_precision_1: 0.9306 - val_recall_1: 0.8933 - lr: 1.5625e-05\n",
      "Epoch 77/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5767 - accuracy: 0.9858 - precision_1: 0.9914 - recall_1: 0.7961\n",
      "Epoch 77: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 820ms/step - loss: 0.5767 - accuracy: 0.9858 - precision_1: 0.9914 - recall_1: 0.7961 - val_loss: 0.7180 - val_accuracy: 0.9200 - val_precision_1: 0.9306 - val_recall_1: 0.8933 - lr: 7.8125e-06\n",
      "Epoch 78/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6292 - accuracy: 0.9830 - precision_1: 0.9900 - recall_1: 0.7571\n",
      "Epoch 78: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 841ms/step - loss: 0.6292 - accuracy: 0.9830 - precision_1: 0.9900 - recall_1: 0.7571 - val_loss: 0.7174 - val_accuracy: 0.9200 - val_precision_1: 0.9306 - val_recall_1: 0.8933 - lr: 7.8125e-06\n",
      "Epoch 79/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5295 - accuracy: 0.9872 - precision_1: 0.9914 - recall_1: 0.8433\n",
      "Epoch 79: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 815ms/step - loss: 0.5295 - accuracy: 0.9872 - precision_1: 0.9914 - recall_1: 0.8433 - val_loss: 0.7128 - val_accuracy: 0.9200 - val_precision_1: 0.9324 - val_recall_1: 0.9200 - lr: 7.8125e-06\n",
      "Epoch 80/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4939 - accuracy: 0.9858 - precision_1: 0.9872 - recall_1: 0.9145\n",
      "Epoch 80: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 922ms/step - loss: 0.4939 - accuracy: 0.9858 - precision_1: 0.9872 - recall_1: 0.9145 - val_loss: 0.7226 - val_accuracy: 0.9200 - val_precision_1: 0.9178 - val_recall_1: 0.8933 - lr: 7.8125e-06\n",
      "Epoch 81/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.9858 - precision_1: 0.9886 - recall_1: 0.8601\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 81: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 852ms/step - loss: 0.4869 - accuracy: 0.9858 - precision_1: 0.9886 - recall_1: 0.8601 - val_loss: 0.7191 - val_accuracy: 0.9200 - val_precision_1: 0.9315 - val_recall_1: 0.9067 - lr: 7.8125e-06\n",
      "Epoch 82/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.9801 - precision_1: 0.9829 - recall_1: 0.8825\n",
      "Epoch 82: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 19s 814ms/step - loss: 0.5063 - accuracy: 0.9801 - precision_1: 0.9829 - recall_1: 0.8825 - val_loss: 0.7218 - val_accuracy: 0.9200 - val_precision_1: 0.9178 - val_recall_1: 0.8933 - lr: 3.9063e-06\n",
      "Epoch 83/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5187 - accuracy: 0.9759 - precision_1: 0.9886 - recall_1: 0.9202\n",
      "Epoch 83: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 842ms/step - loss: 0.5187 - accuracy: 0.9759 - precision_1: 0.9886 - recall_1: 0.9202 - val_loss: 0.7161 - val_accuracy: 0.9200 - val_precision_1: 0.9315 - val_recall_1: 0.9067 - lr: 3.9063e-06\n",
      "Epoch 84/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5711 - accuracy: 0.9816 - precision_1: 0.9872 - recall_1: 0.8398\n",
      "Epoch 84: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 17s 721ms/step - loss: 0.5711 - accuracy: 0.9816 - precision_1: 0.9872 - recall_1: 0.8398 - val_loss: 0.7133 - val_accuracy: 0.9200 - val_precision_1: 0.9315 - val_recall_1: 0.9067 - lr: 3.9063e-06\n",
      "Epoch 85/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4944 - accuracy: 0.9887 - precision_1: 0.9914 - recall_1: 0.8697\n",
      "Epoch 85: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 856ms/step - loss: 0.4944 - accuracy: 0.9887 - precision_1: 0.9914 - recall_1: 0.8697 - val_loss: 0.7082 - val_accuracy: 0.9200 - val_precision_1: 0.9324 - val_recall_1: 0.9200 - lr: 3.9063e-06\n",
      "Epoch 86/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4721 - accuracy: 0.9830 - precision_1: 0.9844 - recall_1: 0.8862Restoring model weights from the end of the best epoch: 71.\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.93333\n",
      "23/23 [==============================] - 20s 856ms/step - loss: 0.4721 - accuracy: 0.9830 - precision_1: 0.9844 - recall_1: 0.8862 - val_loss: 0.7122 - val_accuracy: 0.9200 - val_precision_1: 0.9315 - val_recall_1: 0.9067 - lr: 3.9063e-06\n",
      "Epoch 86: early stopping\n",
      "\n",
      "==================================================\n",
      "EVALUATING MODEL\n",
      "==================================================\n",
      "3/3 [==============================] - 1s 184ms/step\n",
      "\n",
      "Test Accuracy: 0.9247\n",
      "\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "  Bundle branch block     0.4000    0.6667    0.5000         3\n",
      "     Healthy controls     0.9333    0.8750    0.9032        16\n",
      "Myocardial infarction     0.9589    0.9459    0.9524        74\n",
      "\n",
      "             accuracy                         0.9247        93\n",
      "            macro avg     0.7641    0.8292    0.7852        93\n",
      "         weighted avg     0.9365    0.9247    0.9293        93\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 2  0  1]\n",
      " [ 0 14  2]\n",
      " [ 3  1 70]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model and results saved!\n",
      "- Model: models/best_model_multiclass.keras\n",
      "- Confusion Matrix: confusion_matrix_multiclass.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, MaxPooling1D, GlobalAveragePooling1D,\n",
    "    Dense, Dropout, BatchNormalization, Add, Activation\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "base_dir = 'Data/ptb-diagnostic-ecg-database'\n",
    "output_dir = 'Data/processed_signals'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "SIGNAL_LENGTH = 5000\n",
    "NUM_LEADS = 15\n",
    "\n",
    "# Classes to EXCLUDE\n",
    "EXCLUDED_CLASSES = ['Myocarditis', 'Valvular heart disease','Cardiomyopathy/Heart failure','Cardiomyopathy/Heart failure','Dysrhythmia']\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def to_key(s):\n",
    "    s = s.strip('# ').lower()\n",
    "    s = s.replace('(localization)','localization').replace('(at rest)','at_rest').replace('(load)','load')\n",
    "    return re.sub(r'[^a-z0-9]+','_',s).strip('_')\n",
    "\n",
    "def parse_comments(comments):\n",
    "    d = {}\n",
    "    for c in comments or []:\n",
    "        if ':' in c:\n",
    "            k, v = c.split(':', 1)\n",
    "            d[to_key(k)] = v.strip()\n",
    "    return d\n",
    "\n",
    "def map_primary_label(meta):\n",
    "    raw = (meta.get('reason_for_admission') or meta.get('diagnose') or '').lower()\n",
    "    raw = raw.replace('heartfailure','heart failure').replace('arrhythmia','dysrhythmia')\n",
    "    if 'healthy control' in raw: return 'Healthy controls'\n",
    "    if 'myocardial infarction' in raw: return 'Myocardial infarction'\n",
    "    if 'cardiomyopathy' in raw or 'heart failure' in raw: return 'Cardiomyopathy/Heart failure'\n",
    "    if 'bundle branch block' in raw: return 'Bundle branch block'\n",
    "    if 'dysrhythmia' in raw: return 'Dysrhythmia'\n",
    "    if 'myocardial hypertrophy' in raw: return 'Myocardial hypertrophy'\n",
    "    if 'valvular heart disease' in raw: return 'Valvular heart disease'\n",
    "    if 'myocarditis' in raw: return 'Myocarditis'\n",
    "    return 'Other'\n",
    "\n",
    "def load_ecg_safely(record_path, target_length=5000):\n",
    "    try:\n",
    "        record = wfdb.rdrecord(record_path)\n",
    "        signal = record.p_signal\n",
    "        \n",
    "        if signal is None or len(signal) == 0:\n",
    "            return None, None\n",
    "        if signal.shape[1] != NUM_LEADS:\n",
    "            if signal.shape[1] < NUM_LEADS:\n",
    "                padding = NUM_LEADS - signal.shape[1]\n",
    "                signal = np.pad(signal, ((0, 0), (0, padding)), mode='edge')\n",
    "            else:\n",
    "                signal = signal[:, :NUM_LEADS]\n",
    "        \n",
    "        if len(signal) >= target_length:\n",
    "            signal = signal[:target_length, :]\n",
    "        else:\n",
    "            padding = target_length - len(signal)\n",
    "            signal = np.pad(signal, ((0, padding), (0, 0)), mode='edge')\n",
    "        \n",
    "        # Normalization\n",
    "        for i in range(signal.shape[1]):\n",
    "            mean = signal[:, i].mean()\n",
    "            std = signal[:, i].std()\n",
    "            if std > 0:\n",
    "                signal[:, i] = (signal[:, i] - mean) / std\n",
    "            else:\n",
    "                signal[:, i] = 0\n",
    "        \n",
    "        meta = parse_comments(record.comments)\n",
    "        return signal, meta\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD AND PREPROCESS DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading ECG data...\")\n",
    "all_signals = []\n",
    "all_labels = []\n",
    "all_metadata = []\n",
    "\n",
    "patient_folders = sorted([f for f in os.listdir(base_dir) if f.startswith('patient')])\n",
    "total_files = 0\n",
    "loaded_files = 0\n",
    "skipped_files = 0\n",
    "excluded_files = 0\n",
    "\n",
    "print(f\"Found {len(patient_folders)} patient folders\")\n",
    "\n",
    "for idx, patient_folder in enumerate(patient_folders):\n",
    "    patient_path = os.path.join(base_dir, patient_folder)\n",
    "    \n",
    "    if not os.path.isdir(patient_path):\n",
    "        continue\n",
    "    \n",
    "    hea_files = [f for f in os.listdir(patient_path) if f.endswith('.hea')]\n",
    "    total_files += len(hea_files)\n",
    "    \n",
    "    for hea_file in hea_files:\n",
    "        record_name = hea_file.replace('.hea', '')\n",
    "        record_path = os.path.join(patient_path, record_name)\n",
    "        \n",
    "        signal, meta = load_ecg_safely(record_path, target_length=SIGNAL_LENGTH)\n",
    "        \n",
    "        if signal is None:\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "        \n",
    "        primary_label = map_primary_label(meta)\n",
    "        \n",
    "        # Skip \"Other\" category\n",
    "        if primary_label == 'Other':\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "        \n",
    "        # NEW: Skip excluded classes (Myocarditis and Valvular heart disease)\n",
    "        if primary_label in EXCLUDED_CLASSES:\n",
    "            excluded_files += 1\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "        \n",
    "        all_signals.append(signal)\n",
    "        all_labels.append(primary_label)\n",
    "        all_metadata.append(meta)\n",
    "        loaded_files += 1\n",
    "    \n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(patient_folders)} patients... Loaded: {loaded_files}, Skipped: {skipped_files}\")\n",
    "\n",
    "print(f\"\\nTotal files found: {total_files}\")\n",
    "print(f\"Successfully loaded: {loaded_files}\")\n",
    "print(f\"Excluded (Myocarditis & Valvular): {excluded_files}\")\n",
    "print(f\"Skipped (other reasons): {skipped_files - excluded_files}\")\n",
    "print(f\"Success rate: {loaded_files/total_files*100:.1f}%\")\n",
    "\n",
    "# Convert to arrays\n",
    "X = np.array(all_signals)\n",
    "y_multi = np.array(all_labels)\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y_multi.shape}\")\n",
    "\n",
    "# Show label distribution\n",
    "print(f\"\\nLabel distribution:\")\n",
    "unique, counts = np.unique(y_multi, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  {label}: {count} ({count/len(y_multi)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# ENCODE LABELS FOR MULTI-CLASS CLASSIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "# Encode string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_multi)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "y_categorical = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "print(f\"One-hot encoded labels shape: {y_categorical.shape}\")\n",
    "\n",
    "# Save processed data\n",
    "np.save(os.path.join(output_dir, 'X_signals_15lead_multiclass.npy'), X)\n",
    "np.save(os.path.join(output_dir, 'y_multiclass_encoded.npy'), y_encoded)\n",
    "np.save(os.path.join(output_dir, 'y_multiclass_categorical.npy'), y_categorical)\n",
    "np.save(os.path.join(output_dir, 'y_multiclass_labels.npy'), y_multi)\n",
    "\n",
    "# Save label encoder classes\n",
    "np.save(os.path.join(output_dir, 'label_classes.npy'), label_encoder.classes_)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN/VAL/TEST SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SPLITTING DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y_categorical,\n",
    "    test_size=0.2,\n",
    "    stratify=y_encoded,  # Use encoded labels for stratification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Also split the encoded labels for later use\n",
    "y_trainval_encoded = label_encoder.transform(label_encoder.inverse_transform(np.argmax(y_trainval, axis=1)))\n",
    "y_test_encoded = label_encoder.transform(label_encoder.inverse_transform(np.argmax(y_test, axis=1)))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval,\n",
    "    test_size=0.2,\n",
    "    stratify=y_trainval_encoded,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    count = np.sum(np.argmax(y_train, axis=1) == i)\n",
    "    print(f\"  {class_name}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nValidation set: {X_val.shape[0]} samples\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    count = np.sum(np.argmax(y_val, axis=1) == i)\n",
    "    print(f\"  {class_name}: {count} ({count/len(y_val)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set: {X_test.shape[0]} samples\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    count = np.sum(np.argmax(y_test, axis=1) == i)\n",
    "    print(f\"  {class_name}: {count} ({count/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SMOTE FOR CLASS IMBALANCE (Multi-class)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"APPLYING SMOTE FOR CLASS BALANCING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Convert categorical back to encoded for SMOTE\n",
    "y_train_encoded = np.argmax(y_train, axis=1)\n",
    "\n",
    "print(\"\\nBefore SMOTE:\")\n",
    "unique, counts = np.unique(y_train_encoded, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    class_name = label_encoder.classes_[label]\n",
    "    print(f\"  {class_name}: {count}\")\n",
    "\n",
    "# Reshape for SMOTE\n",
    "X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42, k_neighbors=min(3, min(counts)-1))\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_2d, y_train_encoded)\n",
    "\n",
    "# Reshape back\n",
    "X_train_balanced = X_train_balanced.reshape(-1, SIGNAL_LENGTH, NUM_LEADS)\n",
    "\n",
    "# Convert back to categorical\n",
    "y_train_balanced = to_categorical(y_train_balanced, num_classes=num_classes)\n",
    "\n",
    "print(\"\\nAfter SMOTE:\")\n",
    "unique, counts = np.unique(np.argmax(y_train_balanced, axis=1), return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    class_name = label_encoder.classes_[label]\n",
    "    print(f\"  {class_name}: {count}\")\n",
    "\n",
    "print(f\"\\nBalanced training set shape: {X_train_balanced.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATA AUGMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "def augment_ecg_signal(signal):\n",
    "    augmented = signal.copy()\n",
    "    \n",
    "    if np.random.random() < 0.6:\n",
    "        noise_level = np.random.uniform(0.02, 0.05)\n",
    "        noise = np.random.normal(0, noise_level, signal.shape)\n",
    "        augmented += noise\n",
    "    \n",
    "    if np.random.random() < 0.6:\n",
    "        scale_factor = np.random.uniform(0.8, 1.2)\n",
    "        augmented *= scale_factor\n",
    "    \n",
    "    if np.random.random() < 0.4:\n",
    "        shift = np.random.randint(-150, 150)\n",
    "        augmented = np.roll(augmented, shift, axis=0)\n",
    "    \n",
    "    if np.random.random() < 0.4:\n",
    "        time = np.arange(len(signal))\n",
    "        frequency = np.random.uniform(0.001, 0.003)\n",
    "        amplitude = np.random.uniform(0.05, 0.15)\n",
    "        baseline = amplitude * np.sin(2 * np.pi * frequency * time)\n",
    "        augmented += baseline[:, np.newaxis]\n",
    "    \n",
    "    if np.random.random() < 0.25:\n",
    "        num_flips = np.random.randint(1, 5)\n",
    "        flip_leads = np.random.choice(signal.shape[1], size=num_flips, replace=False)\n",
    "        augmented[:, flip_leads] *= -1\n",
    "    \n",
    "    if np.random.random() < 0.3:\n",
    "        crop_size = np.random.randint(4500, 5000)\n",
    "        start = np.random.randint(0, len(signal) - crop_size)\n",
    "        cropped = augmented[start:start+crop_size]\n",
    "        pad_before = np.random.randint(0, 5000 - crop_size + 1)\n",
    "        pad_after = 5000 - crop_size - pad_before\n",
    "        augmented = np.pad(cropped, ((pad_before, pad_after), (0, 0)), mode='edge')\n",
    "    \n",
    "    if np.random.random() < 0.3:\n",
    "        time = np.arange(len(signal))\n",
    "        freq = np.random.choice([50, 60])\n",
    "        amplitude = np.random.uniform(0.03, 0.08)\n",
    "        interference = amplitude * np.sin(2 * np.pi * freq * time / 1000)\n",
    "        augmented += interference[:, np.newaxis]\n",
    "    \n",
    "    if np.random.random() < 0.2:\n",
    "        num_dropout = np.random.randint(1, 3)\n",
    "        dropout_leads = np.random.choice(signal.shape[1], size=num_dropout, replace=False)\n",
    "        augmented[:, dropout_leads] = 0\n",
    "    \n",
    "    if np.random.random() < 0.25:\n",
    "        warp_factor = np.random.uniform(0.95, 1.05)\n",
    "        indices = np.arange(len(signal))\n",
    "        warped_indices = (indices * warp_factor).astype(int)\n",
    "        warped_indices = np.clip(warped_indices, 0, len(signal) - 1)\n",
    "        augmented = augmented[warped_indices]\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "# ============================================================================\n",
    "# DATA GENERATOR\n",
    "# ============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class ECGDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True, augment=False, mixup_alpha=0.2):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        X_batch = self.X[batch_indices].copy()\n",
    "        y_batch = self.y[batch_indices].copy()\n",
    "        \n",
    "        if self.augment:\n",
    "            X_batch = np.array([augment_ecg_signal(x) for x in X_batch])\n",
    "            \n",
    "            if np.random.random() < 0.3 and len(X_batch) > 1:\n",
    "                lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "                shuffle_indices = np.random.permutation(len(X_batch))\n",
    "                X_batch = lam * X_batch + (1 - lam) * X_batch[shuffle_indices]\n",
    "                y_batch = lam * y_batch + (1 - lam) * y_batch[shuffle_indices]\n",
    "        \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "# Create generators\n",
    "train_gen = ECGDataGenerator(\n",
    "    X_train_balanced, y_train_balanced,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    augment=True,\n",
    "    mixup_alpha=0.2\n",
    ")\n",
    "\n",
    "val_gen = ECGDataGenerator(\n",
    "    X_val, y_val,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "test_gen = ECGDataGenerator(\n",
    "    X_test, y_test,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE (Modified for Multi-class)\n",
    "# ============================================================================\n",
    "\n",
    "def residual_block(x, filters, kernel_size=3, dropout_rate=0.3):\n",
    "    shortcut = x\n",
    "    \n",
    "    x = Conv1D(filters, kernel_size, padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = Conv1D(filters, kernel_size, padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same')(shortcut)\n",
    "    \n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def create_resnet_ecg_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Modified for multi-class classification\n",
    "    Changed: num_classes parameter and final activation to 'softmax'\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv1D(64, 7, padding='same', kernel_regularizer=l2(0.001))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = residual_block(x, 64, kernel_size=5, dropout_rate=0.3)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "    \n",
    "    x = residual_block(x, 128, kernel_size=5, dropout_rate=0.3)\n",
    "    x = residual_block(x, 128, kernel_size=5, dropout_rate=0.3)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "    \n",
    "    x = residual_block(x, 256, kernel_size=3, dropout_rate=0.4)\n",
    "    x = residual_block(x, 256, kernel_size=3, dropout_rate=0.4)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "    \n",
    "    x = residual_block(x, 512, kernel_size=3, dropout_rate=0.4)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # CHANGED: softmax activation for multi-class\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# ============================================================================\n",
    "# COMPILE AND TRAIN MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BUILDING AND COMPILING MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Build model\n",
    "model = create_resnet_ecg_model(\n",
    "    input_shape=(SIGNAL_LENGTH, NUM_LEADS),\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Compute class weights\n",
    "class_weights_array = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.arange(num_classes),\n",
    "    y=np.argmax(y_train_balanced, axis=1)\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights_array))\n",
    "\n",
    "print(f\"\\nClass weights:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {class_name}: {class_weights[i]:.3f}\")\n",
    "\n",
    "# Compile model with categorical crossentropy\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',  # CHANGED: for multi-class\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'models/best_model_multiclass.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATING MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Predictions\n",
    "y_pred_proba = model.predict(test_gen)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix - Multi-class Classification')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(label_encoder.classes_))\n",
    "plt.xticks(tick_marks, label_encoder.classes_, rotation=45, ha='right')\n",
    "plt.yticks(tick_marks, label_encoder.classes_)\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             ha=\"center\", va=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_multiclass.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel and results saved!\")\n",
    "print(f\"- Model: models/best_model_multiclass.keras\")\n",
    "print(f\"- Confusion Matrix: confusion_matrix_multiclass.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambienteKarla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
